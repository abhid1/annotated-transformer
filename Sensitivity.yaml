# Fine grained (element-wise) pruning using Automated Gradual Pruner scheduling for PyTorch's example Word Language model.
#
# The README of PyTorch's word language model example code, promises that this configuration will produce a Test perplexity
# of 72.30, while I was only able to get 84.23, so I use that as the baseline for comparison.
#
# time python3 main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied
#
# =========================================================================================
# | End of training | test loss  4.43 | test ppl    84.23
# =========================================================================================
#
# To save you time, you can download a pretrained model from here:
# https://s3-us-west-1.amazonaws.com/nndistiller/agp-pruning/word_language_model/model.emsize1500.nhid1500.dropout065.tied.pt
#
# With the same configuration, and the pruning schedule below, we get comparable perplexity results:
#
# python3 main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied --compress=../../examples/agp-pruning/word_lang_model.schedule_agp.yaml
#

pruners:
  my_pruner:
    class: 'SensitivityPruner'
    sensitivities:
#      encoder.layers.0.feed_forward.w_1.weight:
#      encoder.layers.0.feed_forward.w_2.weight:
#      encoder.layers.1.feed_forward.w_1.weight:
#      encoder.layers.1.feed_forward.w_2.weight:
#      encoder.layers.2.feed_forward.w_1.weight:
#      encoder.layers.2.feed_forward.w_2.weight:
#      encoder.layers.3.feed_forward.w_1.weight:
#      encoder.layers.3.feed_forward.w_2.weight:
#      encoder.layers.4.feed_forward.w_1.weight:
#      encoder.layers.4.feed_forward.w_2.weight:
#      encoder.layers.5.feed_forward.w_1.weight:
#      encoder.layers.5.feed_forward.w_2.weight:
#      decoder.layers.0.feed_forward.w_1.weight:
#      decoder.layers.0.feed_forward.w_2.weight:
#      decoder.layers.1.feed_forward.w_1.weight:
#      decoder.layers.1.feed_forward.w_2.weight:
#      decoder.layers.2.feed_forward.w_1.weight:
#      decoder.layers.2.feed_forward.w_2.weight:
#      decoder.layers.3.feed_forward.w_1.weight:
#      decoder.layers.3.feed_forward.w_2.weight:
#      decoder.layers.4.feed_forward.w_1.weight:
#      decoder.layers.4.feed_forward.w_2.weight:
#      decoder.layers.5.feed_forward.w_1.weight:
#      decoder.layers.5.feed_forward.w_2.weight:
      encoder.layers.0.self_attn.linears.0.weight: .65
      encoder.layers.0.self_attn.linears.1.weight: .65
      encoder.layers.0.self_attn.linears.2.weight: .65
      encoder.layers.0.self_attn.linears.3.weight: .65
      encoder.layers.1.self_attn.linears.0.weight: .65
      encoder.layers.1.self_attn.linears.1.weight: .65
      encoder.layers.1.self_attn.linears.2.weight: .65
      encoder.layers.1.self_attn.linears.3.weight: .65
      encoder.layers.2.self_attn.linears.0.weight: .65
      encoder.layers.2.self_attn.linears.1.weight: .65
      encoder.layers.2.self_attn.linears.2.weight: .65
      encoder.layers.2.self_attn.linears.3.weight: .65
      encoder.layers.3.self_attn.linears.0.weight: .65
      encoder.layers.3.self_attn.linears.1.weight: .65
      encoder.layers.3.self_attn.linears.2.weight: .65
      encoder.layers.3.self_attn.linears.3.weight: .65
      encoder.layers.4.self_attn.linears.0.weight: .65
      encoder.layers.4.self_attn.linears.1.weight: .65
      encoder.layers.4.self_attn.linears.2.weight: .65
      encoder.layers.4.self_attn.linears.3.weight: .65
      encoder.layers.5.self_attn.linears.0.weight: .65
      encoder.layers.5.self_attn.linears.1.weight: .65
      encoder.layers.5.self_attn.linears.2.weight: .65
      encoder.layers.5.self_attn.linears.3.weight: .65
      decoder.layers.0.self_attn.linears.0.weight: .65
      decoder.layers.0.self_attn.linears.1.weight: .65
      decoder.layers.0.self_attn.linears.2.weight: .65
      decoder.layers.0.self_attn.linears.3.weight: .65
      decoder.layers.1.self_attn.linears.0.weight: .65
      decoder.layers.1.self_attn.linears.1.weight: .65
      decoder.layers.1.self_attn.linears.2.weight: .65
      decoder.layers.1.self_attn.linears.3.weight: .65
      decoder.layers.2.self_attn.linears.0.weight: .65
      decoder.layers.2.self_attn.linears.1.weight: .65
      decoder.layers.2.self_attn.linears.2.weight: .65
      decoder.layers.2.self_attn.linears.3.weight: .65
      decoder.layers.3.self_attn.linears.0.weight: .65
      decoder.layers.3.self_attn.linears.1.weight: .65
      decoder.layers.3.self_attn.linears.2.weight: .65
      decoder.layers.3.self_attn.linears.3.weight: .65
      decoder.layers.4.self_attn.linears.0.weight: .65
      decoder.layers.4.self_attn.linears.1.weight: .65
      decoder.layers.4.self_attn.linears.2.weight: .65
      decoder.layers.4.self_attn.linears.3.weight: .65
      decoder.layers.5.self_attn.linears.0.weight: .65
      decoder.layers.5.self_attn.linears.1.weight: .65
      decoder.layers.5.self_attn.linears.2.weight: .65
      decoder.layers.5.self_attn.linears.3.weight: .65
      decoder.layers.0.src_attn.linears.0.weight: .65
      decoder.layers.0.src_attn.linears.1.weight: .65
      decoder.layers.0.src_attn.linears.2.weight: .65
      decoder.layers.0.src_attn.linears.3.weight: .65
      decoder.layers.1.src_attn.linears.0.weight: .65
      decoder.layers.1.src_attn.linears.1.weight: .65
      decoder.layers.1.src_attn.linears.2.weight: .65
      decoder.layers.1.src_attn.linears.3.weight: .65
      decoder.layers.2.src_attn.linears.0.weight: .65
      decoder.layers.2.src_attn.linears.1.weight: .65
      decoder.layers.2.src_attn.linears.2.weight: .65
      decoder.layers.2.src_attn.linears.3.weight: .65
      decoder.layers.3.src_attn.linears.0.weight: .65
      decoder.layers.3.src_attn.linears.1.weight: .65
      decoder.layers.3.src_attn.linears.2.weight: .65
      decoder.layers.3.src_attn.linears.3.weight: .65
      decoder.layers.4.src_attn.linears.0.weight: .65
      decoder.layers.4.src_attn.linears.1.weight: .65
      decoder.layers.4.src_attn.linears.2.weight: .65
      decoder.layers.4.src_attn.linears.3.weight: .65
      decoder.layers.5.src_attn.linears.0.weight: .65
      decoder.layers.5.src_attn.linears.1.weight: .65
      decoder.layers.5.src_attn.linears.2.weight: .65
      decoder.layers.5.src_attn.linears.3.weight: .65

policies:
  - pruner:
      instance_name : 'my_pruner'
    starting_epoch: 0
    ending_epoch: 20
    frequency: 2